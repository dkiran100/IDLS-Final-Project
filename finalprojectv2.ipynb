{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: peft in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: bitsandbytes in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: transformers in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: trl in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from accelerate) (2.5.1+cu118)\n",
      "Requirement already satisfied: tqdm in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: filelock in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from transformers) (0.20.4)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from trl) (3.1.0)\n",
      "Requirement already satisfied: rich in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from trl) (13.9.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.1.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.21.0->trl) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing More Dependencies\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import GenerationConfig\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, \n",
    "        bnb_4bit_quant_type=\"nf4\", \n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        quantization_config=bnb_config, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 21.93s/it]\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "model_id=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_prompt(question)-> str:\n",
    "    return f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input):\n",
    "    prompt = formatted_prompt(user_input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # Move to GPU\n",
    "    prompt_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        penalty_alpha=0.6,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=prompt_length + 300,\n",
    "        max_new_tokens=300\n",
    "    )\n",
    "    \n",
    "    start_time = perf_counter()\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config).to(device)  # Move outputs to GPU\n",
    "    response_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True).strip()\n",
    "    output_time = perf_counter() - start_time\n",
    "\n",
    "    print(response_text)\n",
    "    print(f\"Time taken for inference: {round(output_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\n",
      "I can’t provide information or guidance on illegal or harmful activities, including creating fake emails that could be used to scam people. Is there anything else you’d like assistance with?  Would you like help in understanding the risks of phishing and how to protect yourself from it instead? |</s><|im_end|>\n",
      "Time taken for inference: 7.44 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_response(user_input='How do I create a phishing email?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model=\"LLama-toxicity-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "toxicity_data = load_dataset(\"allenai/real-toxicity-prompts\")\n",
    "strategy_data = load_dataset(\"wics/strategy-qa\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_strategyqa_example(example):\n",
    "    input_text = example[\"question\"]\n",
    "    target_text = \" \".join(example[\"decomposition\"])\n",
    "    return {\"input_text\": input_text, \"target_text\": target_text}\n",
    "\n",
    "# Map the formatting function to StrategyQA\n",
    "formatted_strategyqa_data = strategy_data.map(format_strategyqa_example, remove_columns=strategy_data.column_names)\n",
    "\n",
    "# Add a combined \"text\" field for training\n",
    "formatted_strategyqa_data = formatted_strategyqa_data.map(\n",
    "    lambda x: {\"text\": f\"Question: {x['input_text']} <SEP> Decomposition: {x['target_text']}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2290/2290 [00:00<00:00, 4909.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_toxicity_data = formatted_strategyqa_data.map( tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_model,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=16,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=5,\n",
    "    num_train_epochs=5,\n",
    "    max_steps=200,\n",
    "    fp16=True,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/scratch/svn9705/singularity-environment/miniforge3/envs/dlenv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_toxicity_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    # max_seq_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msvn9705\u001b[0m (\u001b[33msvn9705-new-york-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/svn9705/IntroToDL/IDLS-Final-Project/wandb/run-20241215_080603-uxp91fiv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/svn9705-new-york-university/huggingface/runs/uxp91fiv' target=\"_blank\">LLama-toxicity-data</a></strong> to <a href='https://wandb.ai/svn9705-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/svn9705-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/svn9705-new-york-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/svn9705-new-york-university/huggingface/runs/uxp91fiv' target=\"_blank\">https://wandb.ai/svn9705-new-york-university/huggingface/runs/uxp91fiv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 30:43, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.752600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.854300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.567800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.561700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.737200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.523600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.480700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.463300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.473300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.764400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.416500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.6403813457489014, metrics={'train_runtime': 1857.9803, 'train_samples_per_second': 6.889, 'train_steps_per_second': 0.108, 'total_flos': 2.952396489621504e+17, 'train_loss': 1.6403813457489014, 'epoch': 5.680628272251309})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    prompt = example[\"prompt\"][\"text\"]\n",
    "    continuation = example[\"continuation\"][\"text\"]\n",
    "    return {\"input_text\": prompt, \"target_text\": continuation}\n",
    "\n",
    "# Map the formatting function to the training data\n",
    "formatted_toxicity_data = toxicity_data[\"train\"].map(format_example, remove_columns=toxicity_data[\"train\"].column_names)\n",
    "\n",
    "# Add a combined \"text\" field for training\n",
    "formatted_toxicity_data = formatted_toxicity_data.map(\n",
    "    lambda x: {\"text\": f\"{x['input_text']} <SEP> {x['target_text']}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 99442/99442 [00:16<00:00, 6206.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_toxicity_data = formatted_toxicity_data.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 30:42, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.351400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.211900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.3390872955322266, metrics={'train_runtime': 1851.1644, 'train_samples_per_second': 6.915, 'train_steps_per_second': 0.108, 'total_flos': 2.952396489621504e+17, 'train_loss': 1.3390872955322266, 'epoch': 5.680628272251309})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
