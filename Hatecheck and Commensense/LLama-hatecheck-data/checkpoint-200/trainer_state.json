{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.3948497854077253,
  "eval_steps": 500,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.08583690987124463,
      "grad_norm": 62.79646301269531,
      "learning_rate": 0.0001996917333733128,
      "loss": 24.2058,
      "step": 5
    },
    {
      "epoch": 0.17167381974248927,
      "grad_norm": 103.84671783447266,
      "learning_rate": 0.00019876883405951377,
      "loss": 25.4101,
      "step": 10
    },
    {
      "epoch": 0.2575107296137339,
      "grad_norm": 54.30069351196289,
      "learning_rate": 0.00019723699203976766,
      "loss": 24.8503,
      "step": 15
    },
    {
      "epoch": 0.34334763948497854,
      "grad_norm": 72.25271606445312,
      "learning_rate": 0.00019510565162951537,
      "loss": 23.2887,
      "step": 20
    },
    {
      "epoch": 0.4291845493562232,
      "grad_norm": 81.28648376464844,
      "learning_rate": 0.0001923879532511287,
      "loss": 23.2507,
      "step": 25
    },
    {
      "epoch": 0.5150214592274678,
      "grad_norm": 68.7068099975586,
      "learning_rate": 0.0001891006524188368,
      "loss": 23.8227,
      "step": 30
    },
    {
      "epoch": 0.6008583690987125,
      "grad_norm": 71.37512969970703,
      "learning_rate": 0.00018526401643540922,
      "loss": 21.7994,
      "step": 35
    },
    {
      "epoch": 0.6866952789699571,
      "grad_norm": 75.05928802490234,
      "learning_rate": 0.00018090169943749476,
      "loss": 22.7901,
      "step": 40
    },
    {
      "epoch": 0.7725321888412017,
      "grad_norm": 76.65218353271484,
      "learning_rate": 0.0001760405965600031,
      "loss": 21.1203,
      "step": 45
    },
    {
      "epoch": 0.8583690987124464,
      "grad_norm": 60.83210754394531,
      "learning_rate": 0.00017071067811865476,
      "loss": 21.6726,
      "step": 50
    },
    {
      "epoch": 0.944206008583691,
      "grad_norm": 71.9316635131836,
      "learning_rate": 0.00016494480483301836,
      "loss": 21.3944,
      "step": 55
    },
    {
      "epoch": 1.0171673819742488,
      "grad_norm": 67.06481170654297,
      "learning_rate": 0.00015877852522924732,
      "loss": 16.9749,
      "step": 60
    },
    {
      "epoch": 1.1030042918454936,
      "grad_norm": 97.86504364013672,
      "learning_rate": 0.0001522498564715949,
      "loss": 19.0235,
      "step": 65
    },
    {
      "epoch": 1.1888412017167382,
      "grad_norm": 53.650657653808594,
      "learning_rate": 0.00014539904997395468,
      "loss": 19.5931,
      "step": 70
    },
    {
      "epoch": 1.2746781115879828,
      "grad_norm": 65.0020751953125,
      "learning_rate": 0.000138268343236509,
      "loss": 20.1547,
      "step": 75
    },
    {
      "epoch": 1.3605150214592274,
      "grad_norm": 64.52494049072266,
      "learning_rate": 0.00013090169943749476,
      "loss": 19.003,
      "step": 80
    },
    {
      "epoch": 1.4463519313304722,
      "grad_norm": 69.77254486083984,
      "learning_rate": 0.00012334453638559057,
      "loss": 19.4515,
      "step": 85
    },
    {
      "epoch": 1.5321888412017168,
      "grad_norm": 58.58216094970703,
      "learning_rate": 0.0001156434465040231,
      "loss": 18.4268,
      "step": 90
    },
    {
      "epoch": 1.6180257510729614,
      "grad_norm": 59.35065841674805,
      "learning_rate": 0.0001078459095727845,
      "loss": 18.7417,
      "step": 95
    },
    {
      "epoch": 1.703862660944206,
      "grad_norm": 71.47188568115234,
      "learning_rate": 0.0001,
      "loss": 19.2967,
      "step": 100
    },
    {
      "epoch": 1.7896995708154506,
      "grad_norm": 59.06582260131836,
      "learning_rate": 9.215409042721552e-05,
      "loss": 18.0584,
      "step": 105
    },
    {
      "epoch": 1.8755364806866952,
      "grad_norm": 66.99595642089844,
      "learning_rate": 8.435655349597689e-05,
      "loss": 18.324,
      "step": 110
    },
    {
      "epoch": 1.9613733905579398,
      "grad_norm": 72.58490753173828,
      "learning_rate": 7.66554636144095e-05,
      "loss": 18.3635,
      "step": 115
    },
    {
      "epoch": 2.0343347639484977,
      "grad_norm": 54.57616424560547,
      "learning_rate": 6.909830056250527e-05,
      "loss": 15.2686,
      "step": 120
    },
    {
      "epoch": 2.1201716738197427,
      "grad_norm": 55.522438049316406,
      "learning_rate": 6.173165676349103e-05,
      "loss": 17.19,
      "step": 125
    },
    {
      "epoch": 2.2060085836909873,
      "grad_norm": 64.95954895019531,
      "learning_rate": 5.4600950026045326e-05,
      "loss": 16.9042,
      "step": 130
    },
    {
      "epoch": 2.291845493562232,
      "grad_norm": 63.52146911621094,
      "learning_rate": 4.7750143528405126e-05,
      "loss": 17.3598,
      "step": 135
    },
    {
      "epoch": 2.3776824034334765,
      "grad_norm": 62.713993072509766,
      "learning_rate": 4.12214747707527e-05,
      "loss": 16.5911,
      "step": 140
    },
    {
      "epoch": 2.463519313304721,
      "grad_norm": 54.4163932800293,
      "learning_rate": 3.5055195166981645e-05,
      "loss": 17.451,
      "step": 145
    },
    {
      "epoch": 2.5493562231759657,
      "grad_norm": 60.773231506347656,
      "learning_rate": 2.9289321881345254e-05,
      "loss": 16.9882,
      "step": 150
    },
    {
      "epoch": 2.6351931330472103,
      "grad_norm": 80.78302764892578,
      "learning_rate": 2.3959403439996907e-05,
      "loss": 17.3394,
      "step": 155
    },
    {
      "epoch": 2.721030042918455,
      "grad_norm": 65.342529296875,
      "learning_rate": 1.9098300562505266e-05,
      "loss": 17.13,
      "step": 160
    },
    {
      "epoch": 2.8068669527896994,
      "grad_norm": 65.23717498779297,
      "learning_rate": 1.4735983564590783e-05,
      "loss": 17.4838,
      "step": 165
    },
    {
      "epoch": 2.8927038626609445,
      "grad_norm": 52.99779510498047,
      "learning_rate": 1.0899347581163221e-05,
      "loss": 16.697,
      "step": 170
    },
    {
      "epoch": 2.978540772532189,
      "grad_norm": 60.45241165161133,
      "learning_rate": 7.612046748871327e-06,
      "loss": 16.7925,
      "step": 175
    },
    {
      "epoch": 3.051502145922747,
      "grad_norm": 50.18077087402344,
      "learning_rate": 4.8943483704846475e-06,
      "loss": 14.571,
      "step": 180
    },
    {
      "epoch": 3.1373390557939915,
      "grad_norm": 49.918434143066406,
      "learning_rate": 2.7630079602323442e-06,
      "loss": 16.0234,
      "step": 185
    },
    {
      "epoch": 3.223175965665236,
      "grad_norm": 52.97169876098633,
      "learning_rate": 1.231165940486234e-06,
      "loss": 16.3432,
      "step": 190
    },
    {
      "epoch": 3.3090128755364807,
      "grad_norm": 55.36399459838867,
      "learning_rate": 3.0826662668720364e-07,
      "loss": 16.5201,
      "step": 195
    },
    {
      "epoch": 3.3948497854077253,
      "grad_norm": 48.6408576965332,
      "learning_rate": 0.0,
      "loss": 16.0445,
      "step": 200
    }
  ],
  "logging_steps": 5,
  "max_steps": 200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.919182029113262e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
